{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffPM9vMVQXpn",
        "outputId": "dffece1e-92e9-427e-e6b9-ea4cafdd8fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded dataset with 306 rows and columns: ['Age', 'year', 'axillary nodes detected', 'Class']\n",
            "✅ Loaded 1 rules from /home/babycj/ASCAT_2026_Multi-Length Attractors FDCA as Classifier/rules/haber_rule.txt\n",
            "Accuracy  : 0.7213114754098361\n",
            "Precision : 0.6172727272727273\n",
            "Recall    : 0.5895833333333333\n",
            "F1 Score  : 0.5957115009746589\n",
            "Execution Time (seconds): 0.6180737018585205\n",
            "Rule [0, 0, 5, 5, 2, 5, 0, 9] → Accuracy = 0.7213\n",
            "\n",
            "✅ Results written to rule_accuracy.txt\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ---------- Step 1: CA evolution ----------\n",
        "def evolve_CA(PS, Rule, d, n, left, right):\n",
        "    \"\"\"Evolves the given CA state once and returns the next state.\"\"\"\n",
        "    m = left + right + 1\n",
        "    NS = [0] * n\n",
        "    for i in range(n):\n",
        "        RMT = 0\n",
        "        rng = m - 1\n",
        "        for j in range(i - left, i + right + 1):\n",
        "            RMT += int((d ** rng) * PS[(n + j) % n])\n",
        "            rng -= 1\n",
        "        NS[i] = Rule[RMT]\n",
        "    return NS\n",
        "\n",
        "\n",
        "# ---------- Step 2: Cycle finding ----------\n",
        "def find_cycle(start_state, Rule, d, n, left, right):\n",
        "    \"\"\"Return the cycle (list of states) reached from the starting state.\"\"\"\n",
        "    seen = {}\n",
        "    PS = start_state[:]\n",
        "    step = 0\n",
        "    while True:\n",
        "        state_tuple = tuple(PS)\n",
        "        if state_tuple in seen:\n",
        "            start_idx = seen[state_tuple]\n",
        "            cycle = list(seen.keys())[start_idx:]\n",
        "            return cycle\n",
        "        seen[state_tuple] = step\n",
        "        step += 1\n",
        "        PS = evolve_CA(PS, Rule, d, n, left, right)\n",
        "\n",
        "\n",
        "# ---------- Step 3: Canonical cycle ----------\n",
        "def canonical_cycle_key(cycle):\n",
        "    \"\"\"Convert a cycle (list of states) into a unique rotation-invariant string key.\"\"\"\n",
        "    strs = [\"\".join(map(str, s)) for s in cycle]\n",
        "    rotations = [\"-\".join(strs[i:] + strs[:i]) for i in range(len(strs))]\n",
        "    return min(rotations)\n",
        "\n",
        "\n",
        "# ---------- Step 4: Rule generation ----------\n",
        "def generate_rule(params, d):\n",
        "    left = right = 1\n",
        "    m = left + right + 1\n",
        "    Rule = []\n",
        "    for x in range(d):\n",
        "        for y in range(d):\n",
        "            for z in range(d):\n",
        "                Rule.append((params[0]*x*y*z + params[1]*x*y + params[2]*x*z +\n",
        "                             params[3]*y*z + params[4]*x + params[5]*y +\n",
        "                             params[6]*z + params[7]) % d)\n",
        "    return Rule\n",
        "\n",
        "\n",
        "def compute_accuracy(df, Rule, d=10, left=1, right=1):\n",
        "    \"\"\"Train-test split, classify, and compute accuracy.\n",
        "    Uses ALL attribute columns (assumes last column is the class).\n",
        "    Pads each attribute to the same global max length (leading zeros),\n",
        "    concatenates them and treats the concatenation as the CA configuration.\n",
        "    \"\"\"\n",
        "    # --- Train/test split ---\n",
        "    train_df = df.sample(frac=0.8, random_state=42)\n",
        "    test_df = df.drop(train_df.index)\n",
        "\n",
        "    # # --- Identify attribute columns and class column (assume class is last column) ---\n",
        "    # attr_cols = list(df.columns[:-1])\n",
        "    # class_col = df.columns[-1]\n",
        "\n",
        "    # --- Identify attribute columns and class column (use column named \"Class\") ---\n",
        "    class_col = \"Class\"\n",
        "    attr_cols = [col for col in df.columns if col != class_col]\n",
        "\n",
        "\n",
        "    # --- Helper: keep only digits from string (fallback for messy inputs) ---\n",
        "    def digits_only(x):\n",
        "        s = str(x)\n",
        "        filtered = \"\".join(ch for ch in s if ch.isdigit())\n",
        "        return filtered if filtered != \"\" else \"0\"\n",
        "\n",
        "    # --- Compute global max length across all attributes (as digit-strings) ---\n",
        "    max_len = 0\n",
        "    for val in df[attr_cols].values.flatten():\n",
        "        ln = len(digits_only(val))\n",
        "        if ln > max_len:\n",
        "            max_len = ln\n",
        "    if max_len == 0:\n",
        "        max_len = 1  # safety\n",
        "\n",
        "    # --- Training phase ---\n",
        "    cycle_map = {}\n",
        "    for _, row in train_df.iterrows():\n",
        "        # pad every attribute to global max_len and concatenate in column order\n",
        "        parts = []\n",
        "        for c in attr_cols:\n",
        "            s = digits_only(row[c])\n",
        "            parts.append(s.zfill(max_len))\n",
        "        concat = \"\".join(parts)\n",
        "\n",
        "        PS = [int(ch) for ch in concat]          # initial configuration\n",
        "        n = len(PS)\n",
        "        label = row[class_col]\n",
        "\n",
        "        cycle = find_cycle(PS, Rule, d, n, left, right)\n",
        "        cycle_key = canonical_cycle_key(cycle)\n",
        "        cycle_map.setdefault(cycle_key, []).append(label)\n",
        "\n",
        "    # --- Assign majority label per cycle ---\n",
        "    cycle_labels = {c: max(set(labels), key=labels.count) for c, labels in cycle_map.items()}\n",
        "\n",
        "    # Print cycle→label mapping (you asked for this)\n",
        "    # print(\"\\nCycle → Label mapping:\")\n",
        "    # for c, lbl in cycle_labels.items():\n",
        "    #     print(f\"Cycle: {c} → Label: {lbl}\")\n",
        "\n",
        "    # # --- Testing phase ---\n",
        "    # correct = 0\n",
        "    # for _, row in test_df.iterrows():\n",
        "    #     parts = []\n",
        "    #     for c in attr_cols:\n",
        "    #         s = digits_only(row[c])\n",
        "    #         parts.append(s.zfill(max_len))\n",
        "    #     concat = \"\".join(parts)\n",
        "    #     PS = [int(ch) for ch in concat]\n",
        "    #     n = len(PS)\n",
        "    #     true_label = row[class_col]\n",
        "\n",
        "    #     test_cycle = find_cycle(PS, Rule, d, n, left, right)\n",
        "    #     test_key = canonical_cycle_key(test_cycle)\n",
        "\n",
        "    #     # --- Check if cycle known ---\n",
        "    #     if test_key in cycle_labels:\n",
        "    #         pred = cycle_labels[test_key]\n",
        "    #     else:\n",
        "    #         # Fallback: nearest median rule (as in your original code)\n",
        "    #         cycle_medians = {}\n",
        "    #         for c in cycle_labels.keys():\n",
        "    #             states = c.split(\"-\")\n",
        "    #             values = [int(s) for s in states]\n",
        "    #             median_val = sorted(values)[len(values)//2]\n",
        "    #             cycle_medians[c] = median_val\n",
        "\n",
        "    #         test_states = test_key.split(\"-\")\n",
        "    #         test_values = [int(s) for s in test_states]\n",
        "    #         test_median = sorted(test_values)[len(test_values)//2]\n",
        "\n",
        "    #         nearest_cycle = min(cycle_medians.keys(), key=lambda c: abs(cycle_medians[c] - test_median))\n",
        "    #         pred = cycle_labels[nearest_cycle]\n",
        "\n",
        "    #     if pred == true_label:\n",
        "    #         correct += 1\n",
        "\n",
        "    # accuracy = correct / len(test_df) if len(test_df) > 0 else 0\n",
        "    # return accuracy\n",
        "\n",
        "\n",
        "\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "    # --- Testing phase ---\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        parts = []\n",
        "        for c in attr_cols:\n",
        "            s = digits_only(row[c])\n",
        "            parts.append(s.zfill(max_len))\n",
        "        concat = \"\".join(parts)\n",
        "        PS = [int(ch) for ch in concat]\n",
        "        n = len(PS)\n",
        "        true_label = row[class_col]\n",
        "\n",
        "        test_cycle = find_cycle(PS, Rule, d, n, left, right)\n",
        "        test_key = canonical_cycle_key(test_cycle)\n",
        "\n",
        "        # --- Prediction ---\n",
        "        if test_key in cycle_labels:\n",
        "            pred = cycle_labels[test_key]\n",
        "        else:\n",
        "            # Fallback using nearest median\n",
        "            cycle_medians = {}\n",
        "            for c in cycle_labels.keys():\n",
        "                states = c.split(\"-\")\n",
        "                values = [int(s) for s in states]\n",
        "                median_val = sorted(values)[len(values)//2]\n",
        "                cycle_medians[c] = median_val\n",
        "\n",
        "            test_states = test_key.split(\"-\")\n",
        "            test_values = [int(s) for s in test_states]\n",
        "            test_median = sorted(test_values)[len(test_values)//2]\n",
        "\n",
        "            nearest_cycle = min(cycle_medians.keys(), key=lambda c: abs(cycle_medians[c] - test_median))\n",
        "            pred = cycle_labels[nearest_cycle]\n",
        "\n",
        "        y_true.append(true_label)\n",
        "        y_pred.append(pred)\n",
        "\n",
        "    # --- Compute metrics ---\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    # ✅ Print inside the function (NOT returned)\n",
        "    print(\"Accuracy  :\", accuracy)\n",
        "    print(\"Precision :\", precision)\n",
        "    print(\"Recall    :\", recall)\n",
        "    print(\"F1 Score  :\", f1)\n",
        "\n",
        "    # ✅ Keep your original return (accuracy only)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Step 6: Main ----------\n",
        "def main():\n",
        "    # === Load dataset (.csv) ===\n",
        "    dataset_path = \"/home/babycj/ASCAT_2026_Multi-Length Attractors FDCA as Classifier/Datasets/Haber-man.csv\"   # <-- change this to your file\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    print(f\"✅ Loaded dataset with {len(df)} rows and columns: {list(df.columns)}\")\n",
        "    # df_expanded = df.sample(n=1000, replace=True, random_state=42)\n",
        "    # print(len(df_expanded))\n",
        "    # === Load rule parameters from file ===\n",
        "    param_file = \"/home/babycj/ASCAT_2026_Multi-Length Attractors FDCA as Classifier/rules/haber_rule.txt\"\n",
        "    with open(param_file, \"r\") as f:\n",
        "        param_lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    param_sets = [[int(x) for x in line.split()] for line in param_lines]\n",
        "    print(f\"✅ Loaded {len(param_sets)} rules from {param_file}\")\n",
        "\n",
        "    import random\n",
        "\n",
        "    # Assuming param_sets is already defined\n",
        "    # sampled_params = random.sample(param_sets, 1000)\n",
        "\n",
        "\n",
        "\n",
        "    # === Evaluate each rule ===\n",
        "    results = []\n",
        "    #for params in param_sets:\n",
        "    for params in param_sets:\n",
        "        Rule = generate_rule(params, d=10)\n",
        "        import time\n",
        "\n",
        "        start = time.time()\n",
        "        acc = compute_accuracy(df, Rule, d=10)\n",
        "        end = time.time()\n",
        "\n",
        "        print(\"Execution Time (seconds):\", end - start)\n",
        "\n",
        "        # acc = compute_accuracy(df_expanded, Rule, d=10)\n",
        "        results.append((params, acc))\n",
        "        print(f\"Rule {params} → Accuracy = {acc:.4f}\")\n",
        "        with open(\"Accuracy.txt\", \"a\") as f:\n",
        "          f.write(\" \".join(map(str, params)) + f\"  Accuracy={acc:.4f}\\n\")\n",
        "\n",
        "    # === Write results to file ===\n",
        "    # with open(\"rule_accuracy.txt\", \"w\") as f:\n",
        "    #     for params, acc in results:\n",
        "    #         f.write(\" \".join(map(str, params)) + f\"  Accuracy={acc:.4f}\\n\")\n",
        "\n",
        "    print(\"\\n✅ Results written to rule_accuracy.txt\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwP2oHmRAy_G"
      },
      "source": [
        "Performance Comparison with Existing ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTMedWg7-xU2",
        "outputId": "35e6f003-0534-4856-d32c-bda48c447b6a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Haber-man.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaber-man.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# <-- change this to your file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[1;32m     15\u001b[0m target \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/.pyenv/versions/fdca310/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Haber-man.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, accuracy_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"Haber-man.csv\"  # <-- change this to your file\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Separate features and target\n",
        "target = df['Class']\n",
        "df = df.drop(columns=['Class'])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize data\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# ************************* SVM *************************\n",
        "print(\"*************************SVM (Linear)************************************\")\n",
        "svc = SVC(kernel='linear', C=10.0, random_state=1)\n",
        "svc.fit(X_train_std, y_train)\n",
        "y_pred_svm = svc.predict(X_test_std)\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_svm))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_pred_svm))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_pred_svm))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_pred_svm))\n",
        "\n",
        "\n",
        "# ************************* MultinomialNB *************************\n",
        "print(\"*************************MultinomialNB************************************\")\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_std, y_train)\n",
        "y_pred_nb = nb.predict(X_test_std)\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_nb))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_pred_nb))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_pred_nb))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_pred_nb))\n",
        "\n",
        "\n",
        "# ************************* Decision Tree *************************\n",
        "print(\"***************************DecisionTreeClassifier*********************\")\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train_std, y_train)\n",
        "y_predict_DT = clf.predict(X_test_std)\n",
        "print(confusion_matrix(y_test, y_predict_DT))\n",
        "print(classification_report(y_test, y_predict_DT))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_predict_DT))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_predict_DT))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_predict_DT))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_predict_DT))\n",
        "\n",
        "\n",
        "# ************************* Linear Regression *************************\n",
        "print(\"**********************LinearRegression*********************\")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train_std, y_train)\n",
        "y_predict_LR = reg.predict(X_test_std)\n",
        "\n",
        "# Convert regression output to class labels (rounding)\n",
        "y_predict_LR = (y_predict_LR >= 0.5).astype(int)\n",
        "\n",
        "print(confusion_matrix(y_test, y_predict_LR))\n",
        "print(classification_report(y_test, y_predict_LR))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_predict_LR))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_predict_LR))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_predict_LR))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_predict_LR))\n",
        "\n",
        "\n",
        "# ************************* KNeighborsClassifier *************************\n",
        "print(\"*************************KNeighborsClassifier***************\")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train_std, y_train)\n",
        "y_predict_KNN = neigh.predict(X_test_std)\n",
        "print(confusion_matrix(y_test, y_predict_KNN))\n",
        "print(classification_report(y_test, y_predict_KNN))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_predict_KNN))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_predict_KNN))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_predict_KNN))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_predict_KNN))\n",
        "\n",
        "# ************************* MLPClassifier *************************\n",
        "print(\"*********************MLPClassifier*******************\")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "clf1 = MLPClassifier(random_state=1, max_iter=300)\n",
        "clf1.fit(X_train_std, y_train)\n",
        "y_predict_MLP = clf1.predict(X_test_std)\n",
        "print(confusion_matrix(y_test, y_predict_MLP))\n",
        "print(classification_report(y_test, y_predict_MLP))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_predict_MLP))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_predict_MLP))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_predict_MLP))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_predict_MLP))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSJUAuatVg12"
      },
      "source": [
        "Exectution time of KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQZm5WgBVfj2",
        "outputId": "e2ff95be-d614-4a59-ff92-6b008943d1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************************KNeighborsClassifier***************\n",
            "[[36  8]\n",
            " [12  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.75      0.82      0.78        44\n",
            "           2       0.43      0.33      0.38        18\n",
            "\n",
            "    accuracy                           0.68        62\n",
            "   macro avg       0.59      0.58      0.58        62\n",
            "weighted avg       0.66      0.68      0.66        62\n",
            "\n",
            "Accuracy: 0.677\n",
            "Precision: 0.750\n",
            "Recall: 0.818\n",
            "F1 Score: 0.783\n",
            "Execution Time (seconds): 0.035925865173339844\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, accuracy_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"Haber-man (1).csv\"  # <-- change this to your file\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Separate features and target\n",
        "target = df['Class']\n",
        "df = df.drop(columns=['Class'])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize data\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "# ************************* KNeighborsClassifier *************************\n",
        "print(\"*************************KNeighborsClassifier***************\")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train_std, y_train)\n",
        "y_predict_KNN = neigh.predict(X_test_std)\n",
        "print(confusion_matrix(y_test, y_predict_KNN))\n",
        "print(classification_report(y_test, y_predict_KNN))\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_predict_KNN))\n",
        "print('Precision: %.3f' % precision_score(y_test, y_predict_KNN))\n",
        "print('Recall: %.3f' % recall_score(y_test, y_predict_KNN))\n",
        "print('F1 Score: %.3f' % f1_score(y_test, y_predict_KNN))\n",
        "# acc = compute_accuracy(df, Rule, d=10)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Execution Time (seconds):\", end - start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fdca310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
